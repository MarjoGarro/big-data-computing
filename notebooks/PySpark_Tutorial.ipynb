{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8rVH_wrVFN"
      },
      "source": [
        "# **Preliminaries**\n",
        "\n",
        "Before diving into _this_ tutorial, I would like to suggest having a look at this [link](https://github.com/gtolomei/python-for-datascience#Class-Schedule) to those who are not too familiar with **Python** and **Jupyter Notebook**. In particular, I would recommend them going through _Lecture 1_ to _5_ (included), as those cover most of the material needed to understand the basics of the Python programming language and the Jupyter Notebook environment (which is very similar to Google Colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv"
      },
      "source": [
        "# **PySpark + Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vtwpPGKn9W"
      },
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark, e.g.:\n",
        "#!pip install pyspark==3.2.1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa"
      },
      "source": [
        "## **2.** Import useful PySpark packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8zPg5APmYv"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQOJijlEz3I"
      },
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.** Create Web UI Console"
      ],
      "metadata": {
        "id": "LXIVVHF-ZrVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "P_nquJJkZ906"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Be sure you create your own account at https://dashboard.ngrok.com/login and replace the token string below with yours\n",
        "!ngrok authtoken 2MamtHU170jRTFqA7ai0WZFniY9_825Vvne665fhVDZdRKNHT # Replace with your own authtoken"
      ],
      "metadata": {
        "id": "aHV-6DGRcPY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ],
      "metadata": {
        "id": "Dw66RiCKeZ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ],
      "metadata": {
        "id": "imGZyiJSguni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6"
      },
      "source": [
        "## **5.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqJ5f0JE3BL"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhTN342EEOYZ"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RdUta65ef-C"
      },
      "source": [
        "# **Create PySpark's <code>DataFrame</code> \"manually\"**\n",
        "\n",
        "Let's first create <code>Employee</code> and <code>Department</code> <code>Row</code> instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSFVBM0veB65"
      },
      "source": [
        "# Row is a generic \"record\" object with an ordered collection of fields that can be accessed by index or name\n",
        "# In this case, we use Row to create the schema of other Row objects\n",
        "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
        "\n",
        "# Create some Row instances following the above schema\n",
        "employee_1 = Employee('Basher', 'armbrust', 'bash@edureka.co', 100000)\n",
        "employee_2 = Employee('Daniel', 'meng', 'daniel@stanford.edu', 120000)\n",
        "employee_3 = Employee('Muriel', None, 'muriel@waterloo.edu', 140000)\n",
        "employee_4 = Employee('Rachel', 'wendell', 'rach_3@edureka.co', 160000)\n",
        "employee_5 = Employee('Zach', 'galifianakis', 'zach_g@edureka.co', 160000)\n",
        "\n",
        "# Print out a whole Row instance\n",
        "print(employee_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUgxU2iqw_Ys"
      },
      "source": [
        "# Access and print out the first Row field of `employee_1` by index\n",
        "print(\"Employee 1's first name is: {:s}\".format(employee_1[0]))\n",
        "\n",
        "# Access and print out the `salary` Row field of `employee_4` by name\n",
        "print(\"Employee 4's salary is: {:d}\".format(employee_4.salary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAo72JQOeIU_"
      },
      "source": [
        "# Create another set of Row objects. \n",
        "# This time, schema is defined by means of 2 named attributes, i.e., `id` and `name`.\n",
        "# Each Row object specifies those attribute at creation time \n",
        "# (rather than having a `Department` Row object containing only the schema, as we did before with `Employee`)\n",
        "department_1 = Row(id='123456', name='HR')\n",
        "department_2 = Row(id='789012', name='OPS')\n",
        "department_3 = Row(id='345678', name='FN')\n",
        "department_4 = Row(id='901234', name='DEV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3W9E5KDhOEC"
      },
      "source": [
        "Now, we'll create a <code>DepartmentWithEmployees</code> <code>Row</code> instance from the Employee and Departments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2QuPzyeP-D"
      },
      "source": [
        "departmentWithEmployees_1 = Row(department=department_1, employees=[employee_1, employee_2, employee_5])\n",
        "departmentWithEmployees_2 = Row(department=department_2, employees=[employee_3, employee_4])\n",
        "departmentWithEmployees_3 = Row(department=department_3, employees=[employee_1, employee_4, employee_3])\n",
        "departmentWithEmployees_4 = Row(department=department_4, employees=[employee_2, employee_3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfsO8kZdi8wX"
      },
      "source": [
        "print(departmentWithEmployees_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl9jYpOdjEZq"
      },
      "source": [
        "Let's create our <code>DataFrame</code> from the list of rows above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFs3njYujC7k"
      },
      "source": [
        "departmentsWithEmployees = [departmentWithEmployees_1, \n",
        "                            departmentWithEmployees_2, \n",
        "                            departmentWithEmployees_3,                           \n",
        "                            departmentWithEmployees_4]\n",
        "\n",
        "# Create the Spark's DataFrame object\n",
        "df = spark.createDataFrame(departmentsWithEmployees)\n",
        "# Print out the DataFrame\n",
        "df.show(n=3, truncate=False) # try, playing with the arguments (e.g., truncate=True/False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3mIrbG2mm7X"
      },
      "source": [
        "# **Create PySpark's <code>DataFrame</code> from an input source**\n",
        "\n",
        "Most of the times, data we will be working with are not manually created as above, yet they are stored in various formats like <code>csv</code>, <code>json</code>, <code>xml</code>, or a [Parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html) file. Moreover, data can be loaded either from _local_ or _distributed_ file systems (e.g., HDFS or Amazon S3).\n",
        "\n",
        "Remember, though, that our development environment is Google Colab, which - differently from Jupyter Notebook that usually runs on our local machine - itself runs on the (Google's) cloud infrastructure. As such, \"_local_\" file system **in this case** means local to Google's cloud! In other words, any references to data made within Google Colab is relative to the Google's end side. That is why we need Google's cloud to know how to get our data before we can make use of it.\n",
        "\n",
        "Roughly, there are **2 possible ways** of doing this:\n",
        "- \"pushing\" data to Google Drive;\n",
        "- using an external data stores like Amazon S3.\n",
        "\n",
        "In this class, we will be using the first option, although in very large, real-world production environments the second method will be the preferred way to go.\n",
        "\n",
        "**Additional References:** \n",
        "- More information on how to let Google Colab know how to get data can be found [here](https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8)\n",
        "- An interesting discussion on the performance of various input formats (mostly CSV vs. Parquet) can be found [here](https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0pzpMiOA_L"
      },
      "source": [
        "## **1. Link Google Colab to our Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTta-dCq5kXF"
      },
      "source": [
        "GDRIVE_DIR = \"/content/drive\" # Replace with your own mount point on Google Drive\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\" # Replace with your own home directory\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2022-23/2022-23-BDC/datasets\" # Replace with your own data directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzUd3JdGS1Tp"
      },
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu77QbD2vC_o"
      },
      "source": [
        "## **2. Data Acquisition**\n",
        "\n",
        "Let's see how to download/store a dataset file located at a remote source directly to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-86ZkJ6PjJ"
      },
      "source": [
        "DATASET_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/fifa-players-2020.csv.bz2\"\n",
        "GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fI3wiGvKBl"
      },
      "source": [
        "### **Download dataset file from URL directly to our Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrLDE_4e7KH"
      },
      "source": [
        "import requests\n",
        "\n",
        "\"\"\"\n",
        "This function downloads a file from a specific URL directly to Google Drive.\n",
        "\"\"\"\n",
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200: # Test if everything went ok\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiuGbfyv8vT"
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n",
        "get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI"
      },
      "source": [
        "### **Read dataset file into a Spark Dataframe**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi5Hd60FFcX"
      },
      "source": [
        "fifa_df = spark.read.load(GDRIVE_DATASET_FILE, \n",
        "                           format=\"csv\", \n",
        "                           sep=\",\", \n",
        "                           inferSchema=\"true\", \n",
        "                           header=\"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(fifa_df))"
      ],
      "metadata": {
        "id": "EzGjevjW6Ef9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`pyspark.sql.DataFrame` vs. `pyspark.pandas.DataFrame`**\n",
        "\n",
        "Although similar, standard `pyspark.sql.DataFrame` has a slightly different set of APIs w.r.t. `pandas`.\n",
        "This means that users from pandas and/or PySpark face API compatibility issue sometimes when they work with pandas API on Spark. \n",
        "\n",
        "Since pandas API on Spark does not target 100% compatibility of both pandas and PySpark, users need to do some workaround to port their pandas and/or PySpark codes or get familiar with pandas API on Spark in this case. This page aims to describe it.\n",
        "\n",
        "More information about this can be found [here](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/pandas_pyspark.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "vHDufGED75y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transform `pyspark.sql.DataFrame` to `pyspak.pandas.DataFrame`**"
      ],
      "metadata": {
        "id": "rUTHTcJSAF1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fifa_df_pos = fifa_df.to_pandas_on_spark() # Deprecated\n",
        "# More information available at: https://stackoverflow.com/questions/75519552/how-to-convert-pyspark-dataframe-to-pandas-on-spark-dataframe\n",
        "fifa_df_pos = fifa_df.pandas_api() "
      ],
      "metadata": {
        "id": "mGHTTE5ZAhgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(fifa_df_pos))"
      ],
      "metadata": {
        "id": "N4msbxwcAoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XSp2OuV-o_t"
      },
      "source": [
        "### **Display the first <code>n=5</code> rows of the loaded dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh8hHLtK-x2m"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.show(n=5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas Dataframe\n",
        "fifa_df_pos.head(5)"
      ],
      "metadata": {
        "id": "NavIKJv-OkI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWZa4lMx_n8v"
      },
      "source": [
        "### **Print out the schema of the DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByVEruT-AK8s"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas Dataframe\n",
        "fifa_df_pos.info() "
      ],
      "metadata": {
        "id": "GGHoC5AVOz4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTd8ep9x74H"
      },
      "source": [
        "### **Check the shape of the loaded dataset, i.e., number of rows and columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRyYqeXGA4l"
      },
      "source": [
        "# Spark DataFrame\n",
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(fifa_df.count(), len(fifa_df.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(fifa_df_pos.shape[0], fifa_df_pos.shape[1]))"
      ],
      "metadata": {
        "id": "R0fHR_fcA_cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WC4RPQgyEsB"
      },
      "source": [
        "### **Check the data types of the loaded dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KhtSnvGIwG"
      },
      "source": [
        "# Spark DataFrame\n",
        "print(fifa_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "print(fifa_df_pos.dtypes)"
      ],
      "metadata": {
        "id": "fQEkUKToPWIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CoH6G5nB9hC"
      },
      "source": [
        "### **Describing a particular column**\n",
        "\n",
        "If we want to have a look at the summary of any particular column of a DataFrame, we use the <code>describe</code> method. This method gives us the statistical summary of the given column if not specified, it provides the statistical summary of the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3dC7383CJGy"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Statistical summary of the `age` column\n",
        "fifa_df.describe(\"age\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas PySpark DataFrame\n",
        "# Statistical summary of the `age` column\n",
        "fifa_df_pos.age.describe()\n",
        "# Alternatively:\n",
        "# fifa_df_pos[\"age\"].describe()"
      ],
      "metadata": {
        "id": "4B76emfeBiXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkxU3CziCdHL"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Statistical summary of the `height_cm` column\n",
        "#fifa_df.describe(\"height_cm\").show()\n",
        "#\n",
        "# Pandas PySpark DataFrame\n",
        "fifa_df_pos.height_cm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-by1hDer9_a"
      },
      "source": [
        "## **3. Data Manipulation**\n",
        "\n",
        "Once data has been successfully loaded into a Spark DataFrame, we can start working with it. Most of the operations involve: _selecting_, _filtering_, _sorting_, _grouping_, and compute _aggregate functions_ (e.g., _count_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zKWMAFDcUo"
      },
      "source": [
        "### **Selecting Multiple Columns**\n",
        "\n",
        "If we want to select particular columns from the DataFrame, we use the <code>select</code> method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Rt7iQ6Dsg-"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.select([\"long_name\", \"club\", \"nationality\"]).show(n=10, truncate=False)\n",
        "# Alternatively (no list notation):\n",
        "# fifa_df.select(\"long_name\", \"club\", \"nationality\").show(n=10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas PySpark DataFrame\n",
        "fifa_df_pos[[\"long_name\", \"club\", \"nationality\"]].head(10)"
      ],
      "metadata": {
        "id": "D68u96PlB7PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnSS_NnaE3CN"
      },
      "source": [
        "If we want to select (multiple) **distinct** columns, we will use the <code>distinct</code> method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JNeCynsFBPM"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.select([\"nationality\"]).distinct().show(n=10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DatFrame\n",
        "fifa_df_pos[\"nationality\"].unique().head(10)"
      ],
      "metadata": {
        "id": "XYstFwh-CNhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X18Up7m1IOt1"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.select([\"club\", \"nationality\"]).distinct().show(n=5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DatFrame\n",
        "# NOT SO EASY! Some workarounds are needed: https://www.geeksforgeeks.org/pandas-find-unique-values-from-multiple-columns/"
      ],
      "metadata": {
        "id": "wHDwtiRXRSBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_0l5Rqz6lg"
      },
      "source": [
        "### **Find Duplicates (if any)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgKnTs5qno8M"
      },
      "source": [
        "# Spark DataFrame\n",
        "print(\"The total number of duplicates ages are {:d} out of {:d}\".\n",
        "      format(fifa_df.count() - fifa_df.dropDuplicates([\"age\"]).count(), fifa_df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "print(\"The total number of duplicates ages are {:d} out of {:d}\".\n",
        "      format(fifa_df_pos.shape[0] - fifa_df_pos.drop_duplicates([\"age\"]).shape[0], fifa_df_pos.shape[0]))"
      ],
      "metadata": {
        "id": "68NpRpWURwzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKRiEfM-J305"
      },
      "source": [
        "### **Filtering Data**\n",
        "In order to filter data, according to the condition specified, we use the <code>filter</code> command. Here we are filtering our DataFrame based on the condition that <code>team_jersey_number</code> must be equal to <code>10</code> and then we are calculating how many records/rows are there in the filtered output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLC3QLahKiMH"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.filter(fifa_df.team_jersey_number==10).show(5, truncate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "fifa_df_pos[(fifa_df_pos.team_jersey_number==10)].head()"
      ],
      "metadata": {
        "id": "uFHnwTg2SCsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfMbUQmyK3lF"
      },
      "source": [
        "# Spark DataFrame\n",
        "print(\"The total number of players having jersey #10 are: {:d}\".\n",
        "      format(fifa_df.filter(fifa_df.team_jersey_number==10).count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "print(\"The total number of players having jersey #10 are: {:d}\".\n",
        "      format(fifa_df_pos[(fifa_df_pos.team_jersey_number==10)].shape[0]))"
      ],
      "metadata": {
        "id": "4I59IfVfSYaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DKEf2hbiGBp"
      },
      "source": [
        "### **Filtering Data (using multiple parameters)**\n",
        "We can filter our data based on multiple logical conditions connected together by <code>AND</code> or <code>OR</code> operators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HdX8WhHiXEu"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Filter Italian goalkeepers only\n",
        "fifa_df.filter((fifa_df.team_position==\"GK\") & (fifa_df.nationality==\"Italy\")).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Filter Italian goalkeepers only\n",
        "fifa_df_pos[(fifa_df_pos.team_position==\"GK\") & (fifa_df_pos.nationality==\"Italy\")].head()"
      ],
      "metadata": {
        "id": "5PrZFbH2Cbhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LaF--VKjZVB"
      },
      "source": [
        "### **Sorting Data**\n",
        "\n",
        "We can sort data using the <code>sort</code> method _or_ the <code>orderBy</code> method (which is just an alias of the former)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Rl4LXQj8rz"
      },
      "source": [
        "# Spark DataFrame\n",
        "fifa_df.sort([\"height_cm\", \"weight_kg\"], ascending=[False, False]).show(5)\n",
        "# Alternatively:\n",
        "# fifa_df.orderBy([\"height_cm\", \"weight_kg\"], ascending=[False, False]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "fifa_df_pos.sort_values([\"height_cm\", \"weight_kg\"], ascending=[False, False]).head()"
      ],
      "metadata": {
        "id": "asRjb_V_DVqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA6gYgvikXcZ"
      },
      "source": [
        "### **Filtering + Sorting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fgqiMYBi5_Q"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Filter Italian goalkeepers only, and sort them by height in ascending order\n",
        "fifa_df.filter((fifa_df.team_position==\"GK\") & (fifa_df.nationality==\"Italy\")).sort(\"height_cm\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Filter Italian goalkeepers only, and sort them by height in ascending order\n",
        "fifa_df_pos[(fifa_df_pos.team_position==\"GK\") & (fifa_df_pos.nationality==\"Italy\")].sort_values(\"height_cm\").head()"
      ],
      "metadata": {
        "id": "nURR1U6gD4sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n79e57SRk86A"
      },
      "source": [
        "### **Grouping Data**\n",
        "\n",
        "In order to group data in a <code>DataFrame</code> on the basis of the values of one or more column, the <code>groupBy</code> operator is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YgMFbhqlUbk"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Let's group data by `age`\n",
        "fifa_df.groupby([\"age\"]).count().show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Let's group data by `age`\n",
        "fifa_df_pos.groupby(by=\"age\").age.count().reset_index(name=\"count\").head(10)"
      ],
      "metadata": {
        "id": "VaQ4SFNXEVi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCxCMeLTmBoL"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Let's group data by `age` and sort it according to the resulting count in not-ascending order\n",
        "fifa_df.groupby([\"age\"]).count().sort(\"count\", ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Let's group data by `age` and sort it according to the resulting count in not-ascending order\n",
        "fifa_df_pos.groupby(by=\"age\").age.count().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "00C8ftZWKEAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyt2QXBBmNJI"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Let's group data by `nationality` and `team_position`\n",
        "fifa_df.groupby([\"nationality\", \"team_position\"]).count().sort(\"count\", ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Let's group data by `nationality` and `team_position`\n",
        "fifa_df_pos.groupby(by=[\"nationality\", \"team_position\"]).nationality.count().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "6xhU0BpYLcGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwrE0rckmw3f"
      },
      "source": [
        "# Spark DataFrame\n",
        "# Let's group data by `nationality` and `team_position` (except \"SUB\" and \"RES\")\n",
        "fifa_df.filter((fifa_df.team_position != \"SUB\") & (fifa_df.team_position != \"RES\")).groupby([\"nationality\", \"team_position\"]).count().sort(\"count\", ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Pandas DataFrame\n",
        "# Let's group data by `nationality` and `team_position` (except \"SUB\" and \"RES\")\n",
        "fifa_df_pos[(fifa_df_pos.team_position != \"SUB\") & (fifa_df_pos.team_position != \"RES\")].groupby([\"nationality\", \"team_position\"]).nationality.count().reset_index(name=\"count\").sort_values(\"count\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "kdF3PkN_Nj-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_NGCSJFqWuh"
      },
      "source": [
        "### **Performing SQL Queries**\n",
        "\n",
        "We can also pass SQL queries directly to any DataFrame, for that we need to create a table from the DataFrame using the <code>registerTempTable</code> method and then use  <code>sqlContext.sql()</code> to pass the SQL queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1PD1SVUkbwp"
      },
      "source": [
        "fifa_df.createOrReplaceTempView('fifa_table')\n",
        "spark.sql('select * from fifa_table').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTXRYgDaqpLi"
      },
      "source": [
        "spark.sql('select distinct(club) from fifa_table').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeqMfe4XsJh4"
      },
      "source": [
        "## **4. Data Visualization**\n",
        "\n",
        "A crucial step of the typical _exploratory analysis_ phase is to visually inspect data in order to get a sense of possible insights (e.g., _outliers_, _relationships_, _patterns_).\n",
        "\n",
        "PySpark has some limited plotting functionality built-in with the package `pyspark.pandas.DataFrame`. \n",
        "\n",
        "However, if we want more flexibility with plots, we **must** bring the data out of the Spark (_distributed_) Context and into our _local_ Python session, where we can deal with it using any of Python's many plotting libraries, such as <code>matplotlib</code> and <code>seaborn</code>. Please, refer to _Lecture 10_ available at this [link](https://github.com/gtolomei/python-for-datascience#Class-Schedule) for a deeper understanding of these libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark's built-in plotting\n",
        "# For more information, please visit the following URL: https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.plot.hist.html\n",
        "fifa_df_pos.age.plot.hist()"
      ],
      "metadata": {
        "id": "rAaX9MrWvV3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODeB-j9YsNBY"
      },
      "source": [
        "# The following import allows to use Python's plotting APIs: matplotlib and seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# The following directive is to allow the inline visualization of generated plots\n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMS-LTGt40cm"
      },
      "source": [
        "### **Move Data out from <code>SparkContext</code> into <code>Pandas</code>**\n",
        "\n",
        "This can be achieved by calling the method <code>toPandas</code> on our original Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1JwRONo49KY"
      },
      "source": [
        "# Convert our `fifa_df` Spark DataFrame into the corresponding Pandas DataFrame\n",
        "p_fifa_df = fifa_df.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fe9qgpp9AC0"
      },
      "source": [
        "### **Plot <code>age</code> Distribution using <code>Pandas</code>' built-in plotting functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-TGzXEHw8lF"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "_ = p_fifa_df.age.plot.hist(ax=ax, bins=10, color=\"lightblue\", edgecolor='black', linewidth=1.2)\n",
        "_ = p_fifa_df.age.plot.density(ax=ax, secondary_y=True, color=\"red\")\n",
        "_ = ax.set_xlabel(\"Age\")\n",
        "_ = ax.set_ylabel(\"Frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFy35lx9d5C"
      },
      "source": [
        "### **Plot <code>age</code> Distribution using <code>matplotlib</code>'s plotting functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_pR4M1v9i6L"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "_ = ax.hist(p_fifa_df.age, bins=10, color=\"lightblue\", edgecolor='black', linewidth=1.2)\n",
        "_ = ax\n",
        "_ = ax.set_xlabel(\"Age\")\n",
        "_ = ax.set_ylabel(\"Frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Gde6yd9Mad"
      },
      "source": [
        "### **Plot <code>age</code> Distribution using <code>seaborn</code>'s plotting functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5hADuHg7oPt"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "# _ = sns.distplot(p_fifa_df.age, \n",
        "#                  bins=10, \n",
        "#                  kde=True,\n",
        "#                  ax=ax, \n",
        "#                  color=\"lightblue\", \n",
        "#                  hist_kws={\"edgecolor\":\"k\", \"linewidth\":1.2},\n",
        "#                  kde_kws={\"color\": \"r\", \"lw\": 1.5}\n",
        "#                  )\n",
        "\n",
        "_ = sns.histplot(p_fifa_df.age, \n",
        "                 bins=10, \n",
        "                 kde=True,\n",
        "                 stat=\"density\",\n",
        "                 ax=ax, \n",
        "                 color=\"lightblue\", \n",
        "                 linewidth=1.2,\n",
        "                 )\n",
        "_ = sns.kdeplot(p_fifa_df.age,\n",
        "                ax=ax, \n",
        "                color='crimson',\n",
        "                linewidth=2,\n",
        "                )\n",
        "\n",
        "_ = ax.set_xlabel(\"Age\")\n",
        "_ = ax.set_ylabel(\"Density(%)\")\n",
        "\n",
        "# More information available at: \n",
        "# https://github.com/mwaskom/seaborn/issues/2344\n",
        "# https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1kb_H-zDTbz"
      },
      "source": [
        "### **Boxplot of a single variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBbArKfd7wgv"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
        "\n",
        "_ = sns.boxplot(x=p_fifa_df.height_cm, color=\"g\")\n",
        "# Alternatively:\n",
        "#_ = sns.boxplot(x=\"height_cm\", data=p_fifa_df, color=\"g\")\n",
        "_ = ax.set_xlabel(\"Height (cm)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIdzSnwJDk9T"
      },
      "source": [
        "### **Boxplot combining 2 variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhWOFLjBDtaU"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "_ = sns.boxplot(x=\"team_position\", y=\"height_cm\", data=p_fifa_df)\n",
        "_ = ax.set_xlabel(\"Team Position\")\n",
        "_ = ax.set_ylabel(\"Height (cm)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC9j3BHTD-VE"
      },
      "source": [
        "# Let's restrict the same plot above to the top-k position, escept for \"SUB\" and \"RES\"\n",
        "k = 10\n",
        "# Filter out `SUB` and `RES` players\n",
        "filtered_df = p_fifa_df[(p_fifa_df.team_position != \"SUB\") & (p_fifa_df.team_position != \"RES\")]\n",
        "# Find out what are the top-k roles\n",
        "top_k_roles = filtered_df[\"team_position\"].value_counts()[:k].index.values\n",
        "print(\"The top-{:d} roles are: {:s}\".format(k, \", \".join(top_k_roles)))\n",
        "# Select only those top-k players\n",
        "filtered_df = filtered_df[(filtered_df.team_position.isin(top_k_roles))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR7iITJqEg3S"
      },
      "source": [
        "# Create a 1x1 figure\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "_ = sns.boxplot(x=\"team_position\", y=\"height_cm\", data=filtered_df)\n",
        "_ = ax.set_xlabel(\"Team Position\")\n",
        "_ = ax.set_ylabel(\"Height (cm)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItnESV_DNJ23"
      },
      "source": [
        "### **Pairplot: Showing relationship between 2 or more variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llDotqJIJwCV"
      },
      "source": [
        "_ = sns.pairplot(p_fifa_df[[\"height_cm\", \"weight_kg\"]],\n",
        "                 kind=\"reg\",\n",
        "                 diag_kind=\"hist\", # hist or kde\n",
        "                 diag_kws={\"color\":\"r\", \"alpha\":0.7, \"edgecolor\":\"k\"},     \n",
        "                 height=3\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Additional Resources**"
      ],
      "metadata": {
        "id": "teui20JzC_So"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are plenty of additional resources you may want to look at. For example, this is the official [PySpark tutorial](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc) available on Google Colab. \n",
        "Another \"must-read\" is the following guide on [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/).\n",
        "\n",
        "However, notice that some of this material may be outdated."
      ],
      "metadata": {
        "id": "glluT-iwDJL6"
      }
    }
  ]
}
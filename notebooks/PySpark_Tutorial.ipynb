{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture_03_PySpark_Tutorial.ipynb","provenance":[{"file_id":"1-fYE7uaSyOk1LkQ5mF-_2PT2uIYRIVXn","timestamp":1581962053137}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNPFjc5uyU/oZ6XIOAgQqiQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6r8rVH_wrVFN","colab_type":"text"},"source":["# **Preliminaries**\n","\n","Before diving into _this_ tutorial, I would like to suggest having a look at this [link](https://github.com/gtolomei/python-for-datascience#Class-Schedule) to those who are not too familiar with **Python** and **Jupyter Notebook**. In particular, I would recommend them going through _Lecture 1_ to _5_ (included), as those cover most of the material needed to understand the basics of the Python programming language and the Jupyter Notebook environment (which is very similar to Google Colab)."]},{"cell_type":"markdown","metadata":{"id":"iNQWYxnsMqQv","colab_type":"text"},"source":["# **PySpark + Colab Setup**"]},{"cell_type":"code","metadata":{"id":"cZF7-IhxZ4vM","colab_type":"code","colab":{}},"source":["JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Set path to JAVA_HOME"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFudfLCRNvXT","colab_type":"text"},"source":["## **1.** Install PySpark and related dependencies"]},{"cell_type":"code","metadata":{"id":"l6vtwpPGKn9W","colab_type":"code","colab":{}},"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = JAVA_HOME"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOrmY8FiOMwa","colab_type":"text"},"source":["## **2.** Import useful PySpark packages"]},{"cell_type":"code","metadata":{"id":"Fh8zPg5APmYv","colab_type":"code","colab":{}},"source":["import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbT2rM-4tvnB","colab_type":"text"},"source":["## **3.** Create Spark context"]},{"cell_type":"code","metadata":{"id":"xXQOJijlEz3I","colab_type":"code","colab":{}},"source":["# Create the session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n","\n","# Create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_eRvyl2xwCV6","colab_type":"text"},"source":["## **4.** Check everything is ok"]},{"cell_type":"code","metadata":{"id":"4fqJ5f0JE3BL","colab_type":"code","colab":{}},"source":["spark"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhTN342EEOYZ","colab_type":"code","colab":{}},"source":["sc._conf.getAll()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RdUta65ef-C","colab_type":"text"},"source":["# **Create PySpark's <code>DataFrame</code> \"manually\"**\n","\n","Let's first create <code>Employee</code> and <code>Department</code> <code>Row</code> instances."]},{"cell_type":"code","metadata":{"id":"dSFVBM0veB65","colab_type":"code","colab":{}},"source":["# Row is a generic \"record\" object with an ordered collection of fields that can be accessed by index or name\n","# In this case, we use Row to create the schema of other Row objects\n","Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n","\n","# Create some Row instances following the above schema\n","employee_1 = Employee('Basher', 'armbrust', 'bash@edureka.co', 100000)\n","employee_2 = Employee('Daniel', 'meng', 'daniel@stanford.edu', 120000 )\n","employee_3 = Employee('Muriel', None, 'muriel@waterloo.edu', 140000 )\n","employee_4 = Employee('Rachel', 'wendell', 'rach_3@edureka.co', 160000 )\n","employee_5 = Employee('Zach', 'galifianakis', 'zach_g@edureka.co', 160000 )\n","\n","# Print out a whole Row instance\n","print(employee_3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUgxU2iqw_Ys","colab_type":"code","colab":{}},"source":["# Access and print out the first Row field of `employee_1` by index\n","print(\"Employee 1's first name is: {:s}\".format(employee_1[0]))\n","\n","# Access and print out the `salary` Row field of `employee_4` by name\n","print(\"Employee 4's salary is: {:d}\".format(employee_4.salary))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAo72JQOeIU_","colab_type":"code","colab":{}},"source":["# Create another set of Row objects. \n","# This time, schema is defined by means of 2 named attributes, i.e., `id` and `name`.\n","# Each Row object specifies those attribute at creation time \n","# (rather than having a `Department` Row object containing only the schema, as we did before with `Employee`)\n","department_1 = Row(id='123456', name='HR')\n","department_2 = Row(id='789012', name='OPS')\n","department_3 = Row(id='345678', name='FN')\n","department_4 = Row(id='901234', name='DEV')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3W9E5KDhOEC","colab_type":"text"},"source":["Now, we'll create a <code>DepartmentWithEmployees</code> <code>Row</code> instance from the Employee and Departments."]},{"cell_type":"code","metadata":{"id":"Hg2QuPzyeP-D","colab_type":"code","colab":{}},"source":["departmentWithEmployees_1 = Row(department=department_1, employees=[employee_1, employee_2, employee_5])\n","departmentWithEmployees_2 = Row(department=department_2, employees=[employee_3, employee_4])\n","departmentWithEmployees_3 = Row(department=department_3, employees=[employee_1, employee_4, employee_3])\n","departmentWithEmployees_4 = Row(department=department_4, employees=[employee_2, employee_3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfsO8kZdi8wX","colab_type":"code","colab":{}},"source":["print(departmentWithEmployees_2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kl9jYpOdjEZq","colab_type":"text"},"source":["Let's create our <code>DataFrame</code> from the list of rows above"]},{"cell_type":"code","metadata":{"id":"yFs3njYujC7k","colab_type":"code","colab":{}},"source":["departmentsWithEmployees = [departmentWithEmployees_1, \n","                            departmentWithEmployees_2, \n","                            departmentWithEmployees_3,                           \n","                            departmentWithEmployees_4]\n","\n","# Create the Spark's DataFrame object\n","df = spark.createDataFrame(departmentsWithEmployees)\n","# Print out the DataFrame\n","df.show(n=2, truncate=False) # try, playing with the arguments (e.g., truncate=True/False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3mIrbG2mm7X","colab_type":"text"},"source":["# **Create PySpark's <code>DataFrame</code> from an input source**\n","\n","Most of the times, data we will be working with are not manually created as above, yet they are stored in various formats like <code>csv</code>, <code>json</code>, <code>xml</code>, or a [Parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html) file. Moreover, data can be loaded either from _local_ or _distributed_ file systems (e.g., HDFS or Amazon S3).\n","\n","Remember, though, that our development environment is Google Colab, which - differently from Jupyter Notebook that usually runs on our local machine - itself runs on the (Google's) cloud infrastructure. As such, \"_local_\" file system **in this case** means local to Google's cloud! In other words, any references to data made within Google Colab is relative to the Google's end side. That is why we need Google's cloud to know how to get our data before we can make use of it.\n","\n","Roughly, there are **2 possible ways** of doing this:\n","- \"pushing\" data to Google Drive;\n","- using an external data stores like Amazon S3.\n","\n","In this class, we will be using the first option, although in very large, real world production environments the second method will be the preferred way to go.\n","\n","**Additional References:** \n","- More information on how to let Google Colab know how to get data can be found [here](https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8)\n","- An interesting discussion on the performance of various input formats (mostly CSV vs. Parquet) can be found [here](https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873)"]},{"cell_type":"markdown","metadata":{"id":"on0pzpMiOA_L","colab_type":"text"},"source":["## **1. Link Google Colab to our Google Drive**"]},{"cell_type":"code","metadata":{"id":"NTta-dCq5kXF","colab_type":"code","colab":{}},"source":["GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\n","GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\n","GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2019-20-BDC/datasets\" # Your own data directory"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzUd3JdGS1Tp","colab_type":"code","colab":{}},"source":["# Point Colaboratory to our Google Drive\n","from google.colab import drive\n","\n","drive.mount(GDRIVE_DIR, force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mu77QbD2vC_o","colab_type":"text"},"source":["## **2. Data Acquisition**\n","\n","Let's see how to download/store a dataset file located at a remote source directly to Google Drive."]},{"cell_type":"code","metadata":{"id":"iZ-86ZkJ6PjJ","colab_type":"code","colab":{}},"source":["DATASET_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/fifa-players-2020.csv.bz2\"\n","GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5fI3wiGvKBl","colab_type":"text"},"source":["### **Download dataset file from URL directly to our Google Drive**"]},{"cell_type":"code","metadata":{"id":"kxrLDE_4e7KH","colab_type":"code","colab":{}},"source":["import requests\n","\n","\"\"\"\n","This function downloads a file from a specific URL directly to Google Drive.\n","\"\"\"\n","def get_data(dataset_url, dest, chunk_size=1024):\n","  response = requests.get(dataset_url, stream=True)\n","  if response.status_code == 200: # Test if everything went ok\n","    with open(dest, \"wb\") as file:\n","      for block in response.iter_content(chunk_size=chunk_size): \n","        if block: \n","          file.write(block)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"quiuGbfyv8vT","colab_type":"code","colab":{}},"source":["print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n","get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n","print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DevlrMcPw1ZI","colab_type":"text"},"source":["### **Read dataset file into a Spark Dataframe**\n"]},{"cell_type":"code","metadata":{"id":"qKi5Hd60FFcX","colab_type":"code","colab":{}},"source":["fifa_df = spark.read.load(GDRIVE_DATASET_FILE, \n","                           format=\"csv\", \n","                           sep=\",\", \n","                           inferSchema=\"true\", \n","                           header=\"true\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XSp2OuV-o_t","colab_type":"text"},"source":["### **Display the first <code>n=5</code> rows of the loaded dataset**"]},{"cell_type":"code","metadata":{"id":"Wh8hHLtK-x2m","colab_type":"code","colab":{}},"source":["fifa_df.show(n=5, truncate=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWZa4lMx_n8v","colab_type":"text"},"source":["### **Print out the schema of the DataFrame**"]},{"cell_type":"code","metadata":{"id":"ByVEruT-AK8s","colab_type":"code","colab":{}},"source":["fifa_df.printSchema()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPTd8ep9x74H","colab_type":"text"},"source":["### **Check the shape of the loaded dataset, i.e., number of rows and columns**"]},{"cell_type":"code","metadata":{"id":"JyRyYqeXGA4l","colab_type":"code","colab":{}},"source":["print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(fifa_df.count(), len(fifa_df.columns)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WC4RPQgyEsB","colab_type":"text"},"source":["### **Check the data types of the loaded dataset**"]},{"cell_type":"code","metadata":{"id":"Q3KhtSnvGIwG","colab_type":"code","colab":{}},"source":["print(fifa_df.dtypes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CoH6G5nB9hC","colab_type":"text"},"source":["### **Describing a particular column**\n","\n","If we want to have a look at the summary of any particular column of a DataFrame, we use the <code>describe</code> method. This method gives us the statistical summary of the given column if not specified, it provides the statistical summary of the DataFrame."]},{"cell_type":"code","metadata":{"id":"P3dC7383CJGy","colab_type":"code","colab":{}},"source":["# Statistical summary of the `age` column\n","fifa_df.describe(\"age\").show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkxU3CziCdHL","colab_type":"code","colab":{}},"source":["# Statistical summary of the `height_cm` column\n","fifa_df.describe(\"height_cm\").show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m-by1hDer9_a","colab_type":"text"},"source":["## **3. Data Manipulation**\n","\n","Once data has been successfully loaded into a Spark DataFrame, we can start working with it. Most of the operations involve: _selecting_, _filtering_, _sorting_, _grouping_, and compute _aggregate functions_ (e.g., _count_)."]},{"cell_type":"markdown","metadata":{"id":"T5zKWMAFDcUo","colab_type":"text"},"source":["### **Selecting Multiple Columns**\n","\n","If we want to select particular columns from the DataFrame, we use the <code>select</code> method."]},{"cell_type":"code","metadata":{"id":"q1Rt7iQ6Dsg-","colab_type":"code","colab":{}},"source":["fifa_df.select([\"long_name\", \"club\", \"nationality\"]).show(n=10, truncate=False)\n","# Alternatively (no list notation):\n","# fifa_df.select(\"long_name\", \"club\", \"nationality\").show(n=10, truncate=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wnSS_NnaE3CN","colab_type":"text"},"source":["If we want to select (multiple) **distinct** columns, we will use the <code>distinct</code> method."]},{"cell_type":"code","metadata":{"id":"0JNeCynsFBPM","colab_type":"code","colab":{}},"source":["fifa_df.select([\"nationality\"]).distinct().show(n=10, truncate=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X18Up7m1IOt1","colab_type":"code","colab":{}},"source":["fifa_df.select([\"club\", \"nationality\"]).distinct().show(n=10, truncate=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vA_0l5Rqz6lg","colab_type":"text"},"source":["### **Find Duplicates (if any)**"]},{"cell_type":"code","metadata":{"id":"vgKnTs5qno8M","colab_type":"code","colab":{}},"source":["print(\"The total number of duplicated ages are {:d} out of {:d}\".\n","      format(fifa_df.count() - fifa_df.dropDuplicates([\"age\"]).count(), fifa_df.count()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SKRiEfM-J305","colab_type":"text"},"source":["### **Filtering Data**\n","In order to filter data, according to the condition specified, we use the <code>filter</code> command. Here we are filtering our DataFrame based on the condition that <code>team_jersey_number</code> must be equal to <code>10</code> and then we are calculating how many records/rows are there in the filtered output."]},{"cell_type":"code","metadata":{"id":"GLC3QLahKiMH","colab_type":"code","colab":{}},"source":["fifa_df.filter(fifa_df.team_jersey_number==10).show(truncate=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfMbUQmyK3lF","colab_type":"code","colab":{}},"source":["print(\"The total number of players having jersey #10 are: {:d}\".\n","      format(fifa_df.filter(fifa_df.team_jersey_number==10).count()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DKEf2hbiGBp","colab_type":"text"},"source":["### **Filtering Data (using multiple parameters)**\n","We can filter our data based on multiple logical conditions connected together by <code>AND</code> or <code>OR</code> operators."]},{"cell_type":"code","metadata":{"id":"5HdX8WhHiXEu","colab_type":"code","colab":{}},"source":["# Filter Italian goalkeepers only\n","fifa_df.filter((fifa_df.team_position==\"GK\") & (fifa_df.nationality==\"Italy\")).show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LaF--VKjZVB","colab_type":"text"},"source":["### **Sorting Data**\n","\n","We can sort data using the <code>sort</code> method _or_ the <code>orderBy</code> method (which is just an alias of the former)."]},{"cell_type":"code","metadata":{"id":"r8Rl4LXQj8rz","colab_type":"code","colab":{}},"source":["fifa_df.sort([\"height_cm\", \"weight_kg\"], ascending=[False, False]).show()\n","# Alternatively:\n","# fifa_df.orderBy([\"height_cm\", \"weight_kg\"], ascending=[False, False]).show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mA6gYgvikXcZ","colab_type":"text"},"source":["### **Filtering + Sorting**"]},{"cell_type":"code","metadata":{"id":"0fgqiMYBi5_Q","colab_type":"code","colab":{}},"source":["# Filter Italian goalkeepers only, and sort them by height in ascending order\n","fifa_df.filter((fifa_df.team_position==\"GK\") & (fifa_df.nationality==\"Italy\")).sort(\"height_cm\").show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n79e57SRk86A","colab_type":"text"},"source":["### **Grouping Data**\n","\n","In order to group data in a <code>DataFrame</code> on the basis of the values of one or more column, the <code>groupBy</code> operator is used."]},{"cell_type":"code","metadata":{"id":"_YgMFbhqlUbk","colab_type":"code","colab":{}},"source":["# Let's group data by `age`\n","fifa_df.groupby([\"age\"]).count().show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCxCMeLTmBoL","colab_type":"code","colab":{}},"source":["# Let's group data by `age` and sort it according to the resulting count in not-ascending order\n","fifa_df.groupby([\"age\"]).count().sort(\"count\", ascending=False).show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uyt2QXBBmNJI","colab_type":"code","colab":{}},"source":["# Let's group data by `nationality` and `team_position`\n","fifa_df.groupby([\"nationality\", \"team_position\"]).count().sort(\"count\", ascending=False).show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwrE0rckmw3f","colab_type":"code","colab":{}},"source":["# Let's group data by `nationality` and `team_position` (except \"SUB\" and \"RES\")\n","fifa_df.filter((fifa_df.team_position != \"SUB\") & (fifa_df.team_position != \"RES\")).groupby([\"nationality\", \"team_position\"]).count().sort(\"count\", ascending=False).show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_NGCSJFqWuh","colab_type":"text"},"source":["### **Performing SQL Queries**\n","\n","We can also pass SQL queries directly to any DataFrame, for that we need to create a table from the DataFrame using the <code>registerTempTable</code> method and then use  <code>sqlContext.sql()</code> to pass the SQL queries."]},{"cell_type":"code","metadata":{"id":"E1PD1SVUkbwp","colab_type":"code","colab":{}},"source":["fifa_df.registerTempTable('fifa_table')\n","spark.sql('select * from fifa_table').show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTXRYgDaqpLi","colab_type":"code","colab":{}},"source":["spark.sql('select distinct(club) from fifa_table').show(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeqMfe4XsJh4","colab_type":"text"},"source":["## **4. Data Visualization**\n","\n","A crucial step of the typical _exploratory analysis_ phase is to visually inspect data in order to get a sense of possible insights (e.g., _outliers_, _relationships_, _patterns_).\n","\n","PySpark **does not** have any plotting functionality (yet). If we want to plot something, we **must** bring the data out of the Spark (_distributed_) Context and into our _local_ Python session, where we can deal with it using any of Python's many plotting libraries, such as <code>matplotlib</code> and <code>seaborn</code>. Please, refer to _Lecture 10_ available at this [link](https://github.com/gtolomei/python-for-datascience#Class-Schedule) for a deeper understanding of these libraries."]},{"cell_type":"code","metadata":{"id":"ODeB-j9YsNBY","colab_type":"code","colab":{}},"source":["# The following import allows to use Python's plotting APIs: matplotlib and seaborn\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# The following directive is to allow the inline visualization of generated plots\n","%matplotlib inline "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMS-LTGt40cm","colab_type":"text"},"source":["### **Move Data out from <code>SparkContext</code> into <code>Pandas</code>**\n","\n","This can be achieved by calling the method <code>toPandas</code> on our original Spark DataFrame."]},{"cell_type":"code","metadata":{"id":"V1JwRONo49KY","colab_type":"code","colab":{}},"source":["# Convert our `fifa_df` Spark DataFrame into the corresponding Pandas DataFrame\n","p_fifa_df = fifa_df.toPandas()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Fe9qgpp9AC0","colab_type":"text"},"source":["### **Plot <code>age</code> Distribution using <code>Pandas</code>' built-in plotting functions**\n"]},{"cell_type":"code","metadata":{"id":"l-TGzXEHw8lF","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(8,6))\n","\n","_ = p_fifa_df.age.plot.hist(ax=ax, bins=10, color=\"lightblue\", edgecolor='black', linewidth=1.2)\n","_ = p_fifa_df.age.plot.density(ax=ax, secondary_y=True, color=\"red\")\n","_ = ax.set_xlabel(\"Age\")\n","_ = ax.set_ylabel(\"Frequency\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfFy35lx9d5C","colab_type":"text"},"source":["### **Plot <code>age</code> Distribution using <code>matplotlib</code>'s plotting functions**"]},{"cell_type":"code","metadata":{"id":"r_pR4M1v9i6L","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(8,6))\n","\n","_ = ax.hist(p_fifa_df.age, bins=10, color=\"lightblue\", edgecolor='black', linewidth=1.2)\n","_ = ax\n","_ = ax.set_xlabel(\"Age\")\n","_ = ax.set_ylabel(\"Frequency\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2Gde6yd9Mad","colab_type":"text"},"source":["### **Plot <code>age</code> Distribution using <code>seaborn</code>'s plotting functions**"]},{"cell_type":"code","metadata":{"id":"A5hADuHg7oPt","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(8,6))\n","\n","_ = sns.distplot(p_fifa_df.age, \n","                 bins=10, \n","                 kde=True,\n","                 ax=ax, \n","                 color=\"lightblue\", \n","                 hist_kws={\"edgecolor\":\"k\", \"linewidth\":1.2},\n","                 kde_kws={\"color\": \"r\", \"lw\": 1.5}\n","                 )\n","_ = ax.set_xlabel(\"Age\")\n","_ = ax.set_ylabel(\"Density(%)\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1kb_H-zDTbz","colab_type":"text"},"source":["### **Boxplot of a single variable**"]},{"cell_type":"code","metadata":{"id":"eBbArKfd7wgv","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(6,4))\n","\n","_ = sns.boxplot(x=p_fifa_df.height_cm, color=\"g\")\n","# Alternatively:\n","#_ = sns.boxplot(x=\"height_cm\", data=p_fifa_df, color=\"g\")\n","_ = ax.set_xlabel(\"Height (cm)\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIdzSnwJDk9T","colab_type":"text"},"source":["### **Boxplot combining 2 variables**"]},{"cell_type":"code","metadata":{"id":"VhWOFLjBDtaU","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(8,6))\n","\n","_ = sns.boxplot(x=\"team_position\", y=\"height_cm\", data=p_fifa_df)\n","_ = ax.set_xlabel(\"Team Position\")\n","_ = ax.set_xlabel(\"Height (cm)\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC9j3BHTD-VE","colab_type":"code","colab":{}},"source":["# Let's restrict the same plot above to the top-k position, escept for \"SUB\" and \"RES\"\n","k = 10\n","# Filter out `SUB` and `RES` players\n","filtered_df = p_fifa_df[(p_fifa_df.team_position != \"SUB\") & (p_fifa_df.team_position != \"RES\")]\n","# Find out what are the top-k roles\n","top_k_roles = filtered_df[\"team_position\"].value_counts()[:k].index.values\n","print(\"The top-{:d} roles are: {:s}\".format(k, \", \".join(top_k_roles)))\n","# Select only those top-k players\n","filtered_df = filtered_df[(filtered_df.team_position.isin(top_k_roles))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iR7iITJqEg3S","colab_type":"code","colab":{}},"source":["# Create a 1x1 figure\n","fig, ax = plt.subplots(1,1, figsize=(8,6))\n","\n","_ = sns.boxplot(x=\"team_position\", y=\"height_cm\", data=filtered_df)\n","_ = ax.set_xlabel(\"Team Position\")\n","_ = ax.set_xlabel(\"Height (cm)\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ItnESV_DNJ23","colab_type":"text"},"source":["### **Pairplot: Showing relationship between 2 or more variables**"]},{"cell_type":"code","metadata":{"id":"llDotqJIJwCV","colab_type":"code","colab":{}},"source":["_ = sns.pairplot(p_fifa_df[[\"height_cm\", \"weight_kg\"]],\n","                 kind=\"reg\",\n","                 diag_kind=\"hist\", # hist or kde\n","                 diag_kws={\"color\":\"r\", \"alpha\":0.7, \"edgecolor\":\"k\"},     \n","                 height=3\n","                 )"],"execution_count":0,"outputs":[]}]}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Clustering with PySpark**\n",
        "\n",
        "This notebook illustrates an application of the K-means clustering algorithm (implemented within the PySparks's MLLib) to a dataset of text news, in order to group them into \"coherent\" categories referring to similar topics.\n",
        "\n",
        "From the raw collection of text news documents, we will see how to effectively preprocess them before feeding them as input to K-means.\n",
        "\n",
        "We will emprically select the best value of K using the \"elbow method\", and we will assess the quality of the clusters obtained using standard validity measures (e.g., silhouette coefficient)."
      ],
      "metadata": {
        "id": "YoLTHpWp-Rfb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8rVH_wrVFN"
      },
      "source": [
        "# **Global Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GumGWC6reG6"
      },
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2022-23/2022-23-BDC/datasets\"\n",
        "DATASET_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/all-the-news-1.csv.bz2\"\n",
        "GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]\n",
        "\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "MAX_K_CLUSTERS = 50 # max number of clusters (more on this later...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vtwpPGKn9W"
      },
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1\n",
        "!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa"
      },
      "source": [
        "## **2.** Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8zPg5APmYv"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQOJijlEz3I"
      },
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV1RghvXuwQG"
      },
      "source": [
        "## **4.** Create <code>ngrok</code> tunnel to check the Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB4QVvEQuIXW"
      },
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Be sure you create your own account at https://dashboard.ngrok.com/login and replace the token string below with yours\n",
        "!ngrok authtoken 2MamtHU170jRTFqA7ai0WZFniY9_825Vvne665fhVDZdRKNHT # Replace with your own authtoken"
      ],
      "metadata": {
        "id": "tKGaSV0F7Sh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ],
      "metadata": {
        "id": "ROQzVuNu7ZE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ],
      "metadata": {
        "id": "jwwPauep7bm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0pzpMiOA_L"
      },
      "source": [
        "## **5.** Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzUd3JdGS1Tp"
      },
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6"
      },
      "source": [
        "## **6.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqJ5f0JE3BL"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhTN342EEOYZ"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu77QbD2vC_o"
      },
      "source": [
        "# **Data Acquisition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fI3wiGvKBl"
      },
      "source": [
        "Download dataset file from URL directly to our Google Drive.\n",
        "\n",
        "**NOTE:** This is just a sample of the full <code>all-the-news</code> dataset available from [Kaggle](https://www.kaggle.com/snapcrack/all-the-news); more specifically, it is one of the three files which the dataset is composed of (i.e., <code>articles1.csv</code>)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrLDE_4e7KH"
      },
      "source": [
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiuGbfyv8vT"
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n",
        "get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI"
      },
      "source": [
        "### Read dataset file into a Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi5Hd60FFcX"
      },
      "source": [
        "news_df = spark.read.load(GDRIVE_DATASET_FILE, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.cache()"
      ],
      "metadata": {
        "id": "fTqwy4Bhtsoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTd8ep9x74H"
      },
      "source": [
        "### Check the shape of the loaded dataset, i.e., number of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRyYqeXGA4l"
      },
      "source": [
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(news_df.count(), len(news_df.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WC4RPQgyEsB"
      },
      "source": [
        "### Print out the schema of the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KhtSnvGIwG"
      },
      "source": [
        "news_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pjib1fiylb6"
      },
      "source": [
        "### Display the first 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4v-Z92rGXoe"
      },
      "source": [
        "news_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_0l5Rqz6lg"
      },
      "source": [
        "### Count the number of duplicated news (if any)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgKnTs5qno8M"
      },
      "source": [
        "print(\"The total number of duplicated news are {:d} out of {:d}\".\n",
        "      format(news_df.count() - news_df.dropDuplicates(['content']).count(), news_df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIewdnUy43lH"
      },
      "source": [
        "### Display the top-10 most duplicated news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc85D2XDq6Fo"
      },
      "source": [
        "news_df.groupby([\"content\"]).count().sort(\"count\", ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtcnTISuZlMh"
      },
      "source": [
        "### Remove duplicate news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGHCoGwB9mBB"
      },
      "source": [
        "news_df = news_df.dropDuplicates([\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlbuZU7N9vIK"
      },
      "source": [
        "print(\"The total number of unique news is: {:d}\".format(news_df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELIYdrnk04Yu"
      },
      "source": [
        "### Check for any missing value (i.e., <code>NULL</code>) along <code>content</code> column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AQt_-7z1EH9"
      },
      "source": [
        "news_df.where(col(\"content\").isNull()).count()\n",
        "# Alternatively, using filter:\n",
        "# news_df.filter(news_df.content.isNull()).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQLfLzcDYWe"
      },
      "source": [
        "### Show the corresponding NULL entry/ies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUTcKAUq1gGv"
      },
      "source": [
        "news_df.where(col(\"content\").isNull()).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOlnYMzS4s3w"
      },
      "source": [
        "### Remove <code>NULL</code> entry/ies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw9w2Ovu1zn8"
      },
      "source": [
        "news_df = news_df.na.drop(subset=[\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66pFGdch-BU_"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjt71kS-ZwY"
      },
      "source": [
        "In this example, we are working with text data and our ultimate goal is to cluster news into groups of coherent \"topics\" using one of the clustering algorithms we know (e.g., K-means). This is a specific task of a more general area, which is referred to as _natural language processing_ (NLP).\n",
        "\n",
        "As **preliminary** steps of any NLP task, at least the following pipeline must be executed first:\n",
        "\n",
        "- Text cleaning:\n",
        " - Case normalization (<code>lower</code>) -> convert all text to lower case;\n",
        " - Filter out _leading_ and _trailing_ whitespaces (<code>trim</code>);\n",
        " - Filter out punctuation symbols (<code>regexp_replace</code>);\n",
        " - Filter out any internal extra whitespace resulting from the step above (<code>regexp_replace</code> + <code>trim</code>).\n",
        "- Tokenization (<code>Tokenizer</code>): splitting raw text into a list of individual _tokens_ (i.e., words), typically using whitespace as delimiter \n",
        "- Stopwords removal (<code>StopWordsRemover</code>): removing so-called _stopwords_, namely words that do not contribute to the deeper meaning of the document like \"the\", \"a\", \"me\", etc.\n",
        "- Stemming (<code>SnowballStemmer</code>): reducing each word to its root or base. For example \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prr9bVUydzmI"
      },
      "source": [
        "def clean_text(df, column_name=\"content\"):\n",
        "    \"\"\" \n",
        "    This function takes the raw text data and apply a standard NLP preprocessing pipeline consisting of the following steps:\n",
        "      - Text cleaning\n",
        "      - Tokenization\n",
        "      - Stopwords removal\n",
        "      - Stemming (Snowball stemmer)\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: the input dataframe along with the `cleaned_content` column as the results of the NLP preprocessing pipeline\n",
        "\n",
        "    \"\"\"\n",
        "    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "    # Text preprocessing pipeline\n",
        "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
        "\n",
        "    # 1. Text cleaning\n",
        "    print(\"# 1. Text Cleaning\\n\")\n",
        "    # 1.a Case normalization\n",
        "    print(\"1.a Case normalization:\")\n",
        "    lower_case_news_df = df.select(\"id\", lower(col(column_name)).alias(column_name))\n",
        "    lower_case_news_df.show(10)\n",
        "    # 1.b Trimming\n",
        "    print(\"1.b Trimming:\")\n",
        "    trimmed_news_df = lower_case_news_df.select(\"id\", trim(col(column_name)).alias(column_name))\n",
        "    trimmed_news_df.show(10)\n",
        "    # 1.c Filter out punctuation symbols\n",
        "    print(\"1.c Filter out punctuation:\")\n",
        "    no_punct_news_df = trimmed_news_df.select(\"id\", (regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n",
        "    no_punct_news_df.show(10)\n",
        "    # 1.d Filter out any internal extra whitespace\n",
        "    print(\"1.d Filter out extra whitespaces:\")\n",
        "    cleaned_news_df = no_punct_news_df.select(\"id\", trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n",
        "    cleaned_news_df.show(10)\n",
        "\n",
        "    # 2. Tokenization (split text into tokens)\n",
        "    print(\"# 2. Tokenization:\")\n",
        "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
        "    tokens_df = tokenizer.transform(cleaned_news_df)\n",
        "    tokens_df.show(10)\n",
        "\n",
        "    # 3. Stopwords removal\n",
        "    print(\"# 3. Stopwords removal:\")\n",
        "    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n",
        "    terms_df = stopwords_remover.transform(tokens_df)\n",
        "    terms_df.show(10)\n",
        "\n",
        "    # 4. Stemming (Snowball stemmer)\n",
        "    print(\"# 4. Stemming:\")\n",
        "    stemmer = SnowballStemmer(language=\"english\")\n",
        "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))\n",
        "    terms_stemmed_df.show(10)\n",
        "    \n",
        "    return terms_stemmed_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOITbChE0WRp"
      },
      "source": [
        "clean_news_df = clean_text(news_df)\n",
        "clean_news_df.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYII3XAqCp7"
      },
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "Machine learning techniques cannot work directly on text data; in fact, words must be first converted into some numerical representation which machine learning algorithms can make use of. This process is often known as _embedding_ or _vectorization_.\n",
        "\n",
        "In terms of vectorization, it is important to remember that it isn't merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Moreover, vectors derived from text data are usually high-dimensional. This is because each dimension of the feature space will correspond to a word, and the language in the documents may have thousands of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Ck6WXpAaqs"
      },
      "source": [
        "## TF-IDF\n",
        "In information retrieval, **tf-idf** - short for term frequency-inverse document frequency - is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13RW7AdQBeD_"
      },
      "source": [
        "def extract_tfidf_features(df, column_name=\"terms_stemmed\"):\n",
        "    \"\"\" \n",
        "    This function takes the text data and converts it into a term frequency-inverse document frequency vector\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: dataframe with tf-idf vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing the feature transformation classes for doing TF-IDF \n",
        "    from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "    ## Creating Term Frequency Vector for each word\n",
        "    #cv = CountVectorizer(inputCol=column_name, outputCol=\"tf_features\", vocabSize=1000, minDF=5)\n",
        "    #cvModel = cv.fit(df)\n",
        "    #tf_features_df = cvModel.transform(df)\n",
        "\n",
        "    ## Alternatively to CountVectorizer, use HashingTF\n",
        "    #hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=1000)\n",
        "    #tf_features_df = hashingTF.transform(df)\n",
        "\n",
        "    ## Carrying out Inverse Document Frequency on the TF data\n",
        "    #idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "    #idfModel = idf.fit(tf_features_df)\n",
        "    #tf_idf_features_df = idfModel.transform(tf_features_df)\n",
        "\n",
        "    # USING PIPELINE\n",
        "    cv = CountVectorizer(inputCol=column_name, outputCol=\"tf_features\", vocabSize=1000, minDF=10)\n",
        "    # hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=1000)\n",
        "    idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[cv, idf]) # replace `cv` with `hashingTF` if needed\n",
        "    features = pipeline.fit(df)\n",
        "    tf_idf_features_df = features.transform(df)\n",
        "\n",
        "    return tf_idf_features_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQprR3DjdYDY"
      },
      "source": [
        "tf_idf_df = extract_tfidf_features(clean_news_df)\n",
        "tf_idf_df.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean-up unused variables"
      ],
      "metadata": {
        "id": "iroAmo0bII9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to free-up some RAM\n",
        "import gc\n",
        "\n",
        "del news_df\n",
        "del clean_news_df\n",
        "# ...\n",
        "\n",
        "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
      ],
      "metadata": {
        "id": "PD1xi6IZH1nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcjFWSitdday"
      },
      "source": [
        "tf_idf_df.select(col(\"features\")).show(10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_AVPZudEfZv"
      },
      "source": [
        "### Check and remove any possible zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gus3IFnA7JAC"
      },
      "source": [
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYhRWY_PwHC"
      },
      "source": [
        "### Check if there is any zero-lenght vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZj5ONJPuth"
      },
      "source": [
        "print(\"Total n. of zero-length vectors: {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23FneXsvQJaZ"
      },
      "source": [
        "### Remove zero-lenght vector(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-YCkcO4PrWP"
      },
      "source": [
        "tf_idf_df = tf_idf_df.where(num_nonzeros(\"features\") > 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEAtctNbQNeG"
      },
      "source": [
        "### Double-check there is no more zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TNJ35M0GoBD"
      },
      "source": [
        "print(\"Total n. of zero-length vectors (after removal): {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Garbage collector: collected %d objects\" % (gc.collect()))"
      ],
      "metadata": {
        "id": "91W9fYNwvIev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2DTA2NdfSA"
      },
      "source": [
        "# **K-means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPdq2yVE9LzC"
      },
      "source": [
        "### Function used for running K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1deqVUurtB"
      },
      "source": [
        "def k_means(dataset, \n",
        "            n_clusters, \n",
        "            distance_measure=\"euclidean\", \n",
        "            max_iter=20, \n",
        "            features_col=\"features\", \n",
        "            prediction_col=\"cluster\", \n",
        "            random_seed=RANDOM_SEED):\n",
        "  \n",
        "  from pyspark.ml.clustering import KMeans\n",
        "  from pyspark.ml.feature import Normalizer\n",
        "\n",
        "  print(\"\"\"Training K-means clustering using the following parameters: \n",
        "  - K (n. of clusters) = {:d}\n",
        "  - max_iter (max n. of iterations) = {:d}\n",
        "  - distance measure = {:s}\n",
        "  - random seed = {:d}\n",
        "  \"\"\".format(n_clusters, max_iter, distance_measure, random_seed))\n",
        "\n",
        "  if distance_measure == \"cosine\":\n",
        "      # Normalize inputs to unit-length vectors\n",
        "      dataset = Normalizer(inputCol=features_col, outputCol=features_col+\"_norm\", p=1).transform(dataset)\n",
        "      features_col = features_col+\"_norm\"\n",
        "  # Train a K-means model\n",
        "  kmeans = KMeans(featuresCol=features_col, \n",
        "                   predictionCol=prediction_col, \n",
        "                   k=n_clusters, \n",
        "                   initMode=\"k-means||\", \n",
        "                   initSteps=5, \n",
        "                   tol=0.000001, \n",
        "                   maxIter=max_iter, \n",
        "                   seed=random_seed, \n",
        "                   distanceMeasure=distance_measure)\n",
        "  model = kmeans.fit(dataset)\n",
        "\n",
        "  # Make clusters\n",
        "  clusters_df = model.transform(dataset)\n",
        "\n",
        "  return model, clusters_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3FUnyx9ZWi"
      },
      "source": [
        "### Function used to evaluate obtained clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZTOi7TWvcXJ"
      },
      "source": [
        "def evaluate_k_means(clusters, \n",
        "                     metric_name=\"silhouette\", \n",
        "                     distance_measure=\"squaredEuclidean\", # cosine\n",
        "                     prediction_col=\"cluster\"\n",
        "                     ):\n",
        "  \n",
        "  from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "  \n",
        "  # Evaluate clustering by computing Silhouette score\n",
        "  evaluator = ClusteringEvaluator(metricName=metric_name,\n",
        "                                  distanceMeasure=distance_measure, \n",
        "                                  predictionCol=prediction_col\n",
        "                                  )\n",
        "\n",
        "  return evaluator.evaluate(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select the Best Value of K with the Elbow Method"
      ],
      "metadata": {
        "id": "sYb2BV7JNaJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_clustering():\n",
        "    clustering_results = {}\n",
        "    for k in range(5, MAX_K_CLUSTERS + 1, 5):\n",
        "        print(\"Running K-means using K = {:d}\".format(k))\n",
        "        model, clusters_df = k_means(tf_idf_df, k, max_iter=50, distance_measure=\"cosine\") # Alternatively, distance_measure=\"euclidean\"\n",
        "        silhouette_k = evaluate_k_means(clusters_df, distance_measure=\"cosine\") # Alternatively, distance_measure=\"squaredEuclidean\"\n",
        "        wssd_k = model.summary.trainingCost\n",
        "        print(\"Silhouette coefficient computed with cosine distance: {:.3f}\".format(silhouette_k))\n",
        "        print(\"Within-cluster Sum of Squared Distances (using cosine distance): {:.3f}\".format(wssd_k))\n",
        "        print(\"--------------------------------------------------------------------------------------\")\n",
        "        clustering_results[k] = (silhouette_k, wssd_k)\n",
        "        # Free up memory space at the end of each iteration\n",
        "        del model\n",
        "        del clusters_df\n",
        "        gc.collect()\n",
        "    return clustering_results"
      ],
      "metadata": {
        "id": "pWtUbOOvNhiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_results = do_clustering()"
      ],
      "metadata": {
        "id": "JjmgjmDCzpOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clustering_results(clustering_results):\n",
        "    # load the dictionary into pandas\n",
        "    df = pd.DataFrame.from_dict(clustering_results, orient='index').reset_index()\n",
        "    df.columns = ['K','Silhouette', 'WSSD']\n",
        "    # Create a 1x1 figure\n",
        "    fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "    _ = sns.pointplot(data=df, x=\"K\", y=\"WSSD\", ax=ax, color=\"orangered\")\n",
        "    _ = ax.set_xlabel(\"K\")\n",
        "    _ = ax.set_ylabel(\"WSSD\")\n"
      ],
      "metadata": {
        "id": "JGNn4X6tz5mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_clustering_results(clustering_results)"
      ],
      "metadata": {
        "id": "DrOIVGaQ2uF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1-52c-HtVh"
      },
      "source": [
        "K=100 # Best value of K (whatever this is!)\n",
        "print(\"Running K-means using K = {:d}\".format(K))\n",
        "model, clusters_df = k_means(tf_idf_df, K, max_iter=50, distance_measure=\"cosine\")\n",
        "clusters_df.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect some clusters"
      ],
      "metadata": {
        "id": "Ou7T2HynjlmA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2MLijHVJHIi"
      },
      "source": [
        "clusters_df.groupBy(\"cluster\").count().sort(\"cluster\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqft7DRcHUoG"
      },
      "source": [
        "# Get unique values in the grouping column\n",
        "clusters = sorted([x[0] for x in clusters_df.select(\"cluster\").distinct().collect()])\n",
        "print(\"Cluster IDs: [{:s}]\".format(\", \".join([str(c) for c in clusters])))\n",
        "\n",
        "# Create a filtered DataFrame for each group in a list comprehension\n",
        "cluster_list = [clusters_df.where(clusters_df.cluster == x) for x in clusters]\n",
        "\n",
        "# Show the results\n",
        "for x_id, x in enumerate(cluster_list):\n",
        "  print(\"Showing the first 10 records of cluster ID #{:d}\".format(x_id))\n",
        "  x.select([\"cluster\", \"id\", \"content\"]).show(10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
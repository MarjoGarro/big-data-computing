{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Principal Component Analysis (PCA)**"
      ],
      "metadata": {
        "id": "LjOH6Tg4nYTc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8rVH_wrVFN"
      },
      "source": [
        "# **Global Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GumGWC6reG6"
      },
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/drive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/MyDrive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2022-23/2022-23-BDC/datasets\"\n",
        "DATASET_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/mnist-train.csv.bz2\"\n",
        "GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]\n",
        "\n",
        "RANDOM_SEED = 42 # for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtmWjQUVSvq2"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRW_JLjSvrA"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si82CaUYSvrA"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "# Alternatively, if you want to install a specific version of pyspark:\n",
        "#!pip install pyspark==3.2.1\n",
        "!pip install -U -q PyDrive # To use files that are stored in Google Drive directly (e.g., without downloading them from an external URL)\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOOZptveSvrB"
      },
      "source": [
        "## **2.** Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX7xDYw4SvrB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvwgAvGSvrB"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhi5bmOeSvrB"
      },
      "outputs": [],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"4050\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"PySparkTutorial\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWSfSKX6SvrB"
      },
      "source": [
        "## **4.** Create <code>ngrok</code> tunnel to check the Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9luPyVNSvrB"
      },
      "outputs": [],
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKGaSV0F7Sh5"
      },
      "outputs": [],
      "source": [
        "# Be sure you create your own account at https://dashboard.ngrok.com/login and replace the token string below with yours\n",
        "!ngrok authtoken 2MamtHU170jRTFqA7ai0WZFniY9_825Vvne665fhVDZdRKNHT # Replace with your own authtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROQzVuNu7ZE_"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a ngrok tunnel on the port 4050 where Spark is running\n",
        "port = '4050'\n",
        "public_url = ngrok.connect(port).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwwPauep7bm4"
      },
      "outputs": [],
      "source": [
        "print(\"To access the Spark Web UI console, please click on the following link to the ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo9prfkJSvrB"
      },
      "source": [
        "## **5.** Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au2-MqC-SvrB"
      },
      "outputs": [],
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsgE-WfSvrB"
      },
      "source": [
        "## **6.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X312aarSvrB"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEvgsXRkSvrC"
      },
      "outputs": [],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu77QbD2vC_o"
      },
      "source": [
        "# **Data Acquisition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STPSeQ4zMaPn"
      },
      "source": [
        "## **MNIST Dataset**\n",
        "\n",
        "[MNIST](http://yann.lecun.com/exdb/mnist/) (\"Modified National Institute of Standards and Technology\") is the de facto \"Hello World\" dataset of computer vision. Since its release in 1999, this classic dataset of handwritten digit images has served as the basis for benchmarking classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fI3wiGvKBl"
      },
      "source": [
        "### Download dataset file from URL directly to our Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrLDE_4e7KH"
      },
      "source": [
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiuGbfyv8vT"
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n",
        "get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI"
      },
      "source": [
        "### Read dataset file into a Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi5Hd60FFcX"
      },
      "source": [
        "mnist_df = spark.read.load(GDRIVE_DATASET_FILE, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTd8ep9x74H"
      },
      "source": [
        "### Check the shape of the loaded dataset, i.e., number of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRyYqeXGA4l"
      },
      "source": [
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(mnist_df.count(), len(mnist_df.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WC4RPQgyEsB"
      },
      "source": [
        "### Print out the schema of the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KhtSnvGIwG"
      },
      "source": [
        "mnist_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXlcRfzmNNCv"
      },
      "source": [
        "### Dataset Shape and Schema\n",
        "\n",
        "The dataset has 785 columns. The first column, called `label`, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated 28-by-28 pixels image, i.e., a 784-dimensional vector.\n",
        "\n",
        "Each pixel column in the dataset has a name like <code>pixel**k**</code>, where **`k`** is an integer between 0 and 783, inclusive. \n",
        "To locate this pixel on the image, suppose that we have decomposed **`k`** as **`k`** = `i * 28 + j`, where `i` and `j` are integers between 0 and 27, inclusive. Then <code>pixel**k**</code> is located on row `i` and column `j` of a 28 x 28 matrix, (indexing by zero).\n",
        "\n",
        "For example, <code>pixel**31**</code> indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pjib1fiylb6"
      },
      "source": [
        "### Display the first 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4v-Z92rGXoe"
      },
      "source": [
        "mnist_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66pFGdch-BU_"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxMxZYeOWGVZ"
      },
      "source": [
        "### Assembling Features into a single column\n",
        "\n",
        "We make use of the `VectorAssembler` transformer, which combines a given list of columns into a single vector column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP79D5otXIHk"
      },
      "source": [
        "columns = ['pixel{:d}'.format(k) for k in range(784)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNg2cRfUaF0L"
      },
      "source": [
        "columns[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIcWzsjmWYMn"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=columns, \n",
        "                            outputCol=\"features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNrAcjPxXYgp"
      },
      "source": [
        "mnist_df = assembler.transform(mnist_df)\n",
        "mnist_df.select(\"features\", \"label\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykyuMhkrdGKR"
      },
      "source": [
        "### Eventually, retain only 2 columns: assembled features and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZs3jjvNZRN-"
      },
      "source": [
        "mnist_df = mnist_df.select(\"features\", \"label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl92tvdj7Kev"
      },
      "source": [
        "# **Principal Component Analysis (PCA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtvqyK51rrGb"
      },
      "source": [
        "mnist_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miMmNWkgCtKo"
      },
      "source": [
        "## **Standardize features**\n",
        "\n",
        "As we know, PCA is highly sensitive to feature scale. Let's standardize each feature so that each feature value $x$ is transformed into $z$, as follows:\n",
        "$$\n",
        "  z = \\frac{x-\\mu}{\\sigma}\n",
        "$$\n",
        "where $\\mu$ is the sample mean of the feature, and $\\sigma$ is the unbiased sample standard deviation (computed from all the observations).\n",
        "In such a way, each $z$ will have $0$-mean and $1$-standard-deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTHlg3IPrJV4"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", \n",
        "                        outputCol=\"std_features\",\n",
        "                        withStd=True, withMean=True)\n",
        "\n",
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel = scaler.fit(mnist_df)\n",
        "\n",
        "# Normalize each feature to have unit standard deviation.\n",
        "mnist_df = scalerModel.transform(mnist_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvVTVijJr8k5"
      },
      "source": [
        "mnist_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXuaAlUPTcdW"
      },
      "source": [
        "## **Run PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th0qsp2hbdGK"
      },
      "source": [
        "K = 10 # number of principal components to extract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVF_bx-v7oRz"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "pca_model = PCA(k=K, inputCol=\"std_features\", outputCol=\"pca_features\")\n",
        "pca_features = pca_model.fit(mnist_df)\n",
        "pca_mnist_df = pca_features.transform(mnist_df).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTkJMPL2_eIQ"
      },
      "source": [
        "pca_mnist_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1ZnV_LDjS9Y"
      },
      "source": [
        "## **Plotting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq6JtU3NaC4G"
      },
      "source": [
        "### Show the proportion of the data variance carried by each principal component (eigenvector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yMj2ZJKYnxY"
      },
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
        "_ = sns.barplot(x=[i for i in range(K)], \n",
        "                y=pca_features.explainedVariance.values, # `explainedVariance` returns the distribution of variance across eigenvectors, i.e., lambda_i/sum lambda_i\n",
        "                ax=ax)\n",
        "_ = ax.set_xlabel(\"Eigenvalues\", labelpad=16, fontsize=16)\n",
        "_ = ax.set_ylabel(\"Proportion of Variance\", fontsize=16)\n",
        "_ = ax.set_xticklabels([\"lambda_{:d}\".format(i) for i in range(K)], rotation=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BcdlEdOM3Jo"
      },
      "source": [
        "### Transform the PySpark DataFrame into Pandas' DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHkMQOfNAe-"
      },
      "source": [
        "pca_mnist_pdf = pca_mnist_df.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_ZPInkPjdWq"
      },
      "source": [
        "pca_mnist_pdf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTeSniEdhK4"
      },
      "source": [
        "### Plot data projected on the 2 principal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSBx4JXojaM5"
      },
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(12,8))\n",
        "_ = plt.scatter(\n",
        "    pca_mnist_pdf.pca_features.map(lambda x: x[0]), \n",
        "    pca_mnist_pdf.pca_features.map(lambda x: x[1]),\n",
        "    c=pca_mnist_pdf.label, \n",
        "    edgecolor='none', \n",
        "    alpha=0.8,\n",
        "    cmap=plt.cm.get_cmap('tab10', 10),\n",
        "    axes=ax\n",
        "    )\n",
        "_ = ax.set_xlabel('PCA 1', labelpad=20, fontsize = 16)\n",
        "_ = ax.set_ylabel('PCA 2', fontsize = 16)\n",
        "plt.colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2DTA2NdfSA"
      },
      "source": [
        "# **K-means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPdq2yVE9LzC"
      },
      "source": [
        "### Function used for running K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1deqVUurtB"
      },
      "source": [
        "def k_means(dataset, \n",
        "            n_clusters, \n",
        "            distance_measure=\"euclidean\", \n",
        "            max_iter=20, \n",
        "            features_col=\"features\", \n",
        "            prediction_col=\"cluster\", \n",
        "            random_seed=RANDOM_SEED):\n",
        "  \n",
        "  from pyspark.ml.clustering import KMeans\n",
        "\n",
        "  print(\"\"\"Training K-means clustering using the following parameters: \n",
        "  - K (n. of clusters) = {:d}\n",
        "  - max_iter (max n. of iterations) = {:d}\n",
        "  - distance measure = {:s}\n",
        "  - random seed = {:d}\n",
        "  \"\"\".format(n_clusters, max_iter, distance_measure, random_seed))\n",
        "  # Train a K-means model\n",
        "  kmeans = KMeans(featuresCol=features_col, \n",
        "                   predictionCol=prediction_col, \n",
        "                   k=n_clusters, \n",
        "                   initMode=\"k-means||\", \n",
        "                   initSteps=5, \n",
        "                   tol=0.000001, \n",
        "                   maxIter=max_iter, \n",
        "                   seed=random_seed, \n",
        "                   distanceMeasure=distance_measure)\n",
        "  model = kmeans.fit(dataset)\n",
        "\n",
        "  # Make clusters\n",
        "  clusters_df = model.transform(dataset)\n",
        "\n",
        "  return model, clusters_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3FUnyx9ZWi"
      },
      "source": [
        "### Function used to evaluate obtained clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZTOi7TWvcXJ"
      },
      "source": [
        "def evaluate_k_means(clusters, \n",
        "                     metric_name=\"silhouette\", \n",
        "                     distance_measure=\"squaredEuclidean\", # cosine\n",
        "                     prediction_col=\"cluster\"\n",
        "                     ):\n",
        "  \n",
        "  from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "  \n",
        "  # Evaluate clustering by computing Silhouette score\n",
        "  evaluator = ClusteringEvaluator(metricName=metric_name,\n",
        "                                  distanceMeasure=distance_measure, \n",
        "                                  predictionCol=prediction_col\n",
        "                                  )\n",
        "\n",
        "  return evaluator.evaluate(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1p5kISG9et-"
      },
      "source": [
        "### Run K-means by calling the function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgvwHoeBvmzY"
      },
      "source": [
        "model, clusters_df = k_means(pca_mnist_df, \n",
        "                             10,\n",
        "                             distance_measure=\"euclidean\",\n",
        "                             max_iter=1000, \n",
        "                             features_col=\"pca_features\"\n",
        "                             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYu_vheKwTc0"
      },
      "source": [
        "evaluate_k_means(clusters_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1-52c-HtVh"
      },
      "source": [
        "clusters_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2MLijHVJHIi"
      },
      "source": [
        "clusters_df.groupBy(\"cluster\").count().sort(\"cluster\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqft7DRcHUoG"
      },
      "source": [
        "# Get unique values in the grouping column\n",
        "clusters = sorted([x[0] for x in clusters_df.select(\"cluster\").distinct().collect()])\n",
        "print(\"Cluster IDs: [{:s}]\".format(\", \".join([str(c) for c in clusters])))\n",
        "\n",
        "# Create a filtered DataFrame for each group in a list comprehension\n",
        "cluster_list = [clusters_df.where(clusters_df.cluster == x) for x in clusters]\n",
        "\n",
        "# Show the results\n",
        "for x_id, x in enumerate(cluster_list):\n",
        "  print(\"Showing the first 10 records of cluster ID #{:d}\".format(x_id))\n",
        "  x.select([\"cluster\", \"label\"]).show(10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
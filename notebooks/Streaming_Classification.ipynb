{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Lecture_16_Streaming_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0S4veb_86UU",
        "colab_type": "text"
      },
      "source": [
        "# Real-Time Twitter Hate Speech Detection\n",
        "\n",
        "Differently from the notebooks we have seen so far, this one assumes we will be working on our own local installation of Spark rather than the one (still local!) provided by some machine on the Google Cloud infrastructure.\n",
        "\n",
        "The aim of this notebook is to implement a Spark streaming application to detect hate speech in tweets posted on Twitter in (nearly) real-time. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it.\n",
        "\n",
        "So, ultimately the task is to classify racist or sexist tweets from an incoming streaming of tweets. We will use a training dataset of tweets along with their associated labels, where label `1` denotes a racist/sexist tweet and label `0` indicates a legitimate tweet.\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/twitter-mute-filter.png)\n",
        "\n",
        "Why is this a relevant task? Because social media platforms like Twitter receive mammoth streaming data in the form of comments and status updates; this application will help us moderate what is being posted publicly.\n",
        "\n",
        "More details on this task can be found here: [Practice Problem: Twitter Sentiment Analysis](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=blog&utm_medium=streaming-data-pyspark-machine-learning-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwMZbZ4e86UV",
        "colab_type": "text"
      },
      "source": [
        "# A Quick Note on the Dataset\n",
        "\n",
        "The original dataset used for this task is available [here](https://github.com/lakshay-arora/PySpark/blob/master/spark_streaming/datasets/twitter_sentiments.csv), whilst a copy of it is also available on the course's [website](https://github.com/gtolomei/big-data-computing/raw/master/datasets/twitter-sentiments.csv.bz2).\n",
        "\n",
        "Such a dataset contains **31,962** tweets: **29,720** of them (i.e., ~93%) are labelled as _negative_ (`0`), and the remaining **2,242** (i.e., ~7%) are labelled as _positive_ (`1`). In order to deal with such a high skewness, the dataset has been split into **2** portions using **stratified random sampling** (which maintains the same class distributions on both portions): a _training set_ which accounts for 90% of the original instances and a _test set_ which contains the remaining 10% of the instances. The former is used for training a machine learning model for hate speech detection, whilst the latter is used to test the model simulating a stream of incoming tweets.\n",
        "\n",
        "The training set is available [here](https://github.com/gtolomei/big-data-computing/raw/master/datasets/twitter-sentiments-train.csv.bz2), and the test set is available [here](https://github.com/gtolomei/big-data-computing/raw/master/datasets/twitter-sentiments-test.csv.bz2).\n",
        "\n",
        "In addition to that, the test set has been further processed in order to easily simulate the incoming stream of tweets. Roughly, for each tweet in the test set a record is created as follows:\n",
        "\n",
        "`TWEET \"${TWEET_TEXT_HERE}\"`\n",
        "\n",
        "The usage of the token `TWEET` is used by the streaming application for delimiting and separating individual tweets when they are streamed out. This file is also available from [here](https://github.com/gtolomei/big-data-computing/raw/master/datasets/twitter-sentiments-stream.txt)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLeU4Avi86UW",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the Workflow\n",
        "\n",
        "The development of our streaming application is composed of the following stages:\n",
        "\n",
        "-  **Model Building:** This step is devoted to the (<i>offline</i>) training of a machine learning model, which is able to classify between tweets containing hate speech or not; more precisely, we will build a **logistic regression** pipeline. Note that, our focus here is not to build a very accurate classification model but to see how to use _any_ model to make nearly real-time predictions on streaming data;\n",
        "-  **Spark Streaming Context Initialization:** Once the model is built, we need to define the `hostname` and `port number` from where we get the streaming data;\n",
        "-  **Streaming Data:** Next, we will add the tweets from the [`netcat`](http://netcat.sourceforge.net/) server from the defined port, and the Spark Streaming API will receive the data after a specified duration (i.e., <i>batch interval</i>);\n",
        "-  **Real-Time Prediction:** Once we receive the tweet text, we pass it into the machine learning pipeline we created and return the predicted sentiment from the model.\n",
        "\n",
        "Here's a neat illustration of our workflow:\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4QwNi_D86UW",
        "colab_type": "text"
      },
      "source": [
        "## Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wYXkSSS86UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOCAL_SPARK_HOME = \"/usr/local/spark\" # change this to your own local Spark installation directory\n",
        "APP_NAME = \"PySparkTweetHateSpeechDetector\"\n",
        "HOSTNAME = \"localhost\"\n",
        "PORT = 9876\n",
        "DATASET_FILENAME = \"twitter-sentiments-train.csv.bz2\"\n",
        "GDRIVE_DATASET_DIR = \"/Users/gabriele/Google\\ Drive\\ UniRoma\\ \\[@di.uniroma1.it\\]/Teaching/2019-20-BDC/datasets\" # change this to your own local path to the dataset file\n",
        "DATASET_FILE_PATH = GDRIVE_DATASET_DIR + \"/\" + DATASET_FILENAME\n",
        "BATCH_INTERVAL_SECS = 3\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xxgFib86Ua",
        "colab_type": "text"
      },
      "source": [
        "## Import `findspark` to Locate Local Spark Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbVpSudT86Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(LOCAL_SPARK_HOME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3dQzxjf86Ud",
        "colab_type": "text"
      },
      "source": [
        "## Import all the other Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Jkr_Xe86Ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pyspark.sql.types as tp\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import Row, Column\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05Fechx86Uf",
        "colab_type": "text"
      },
      "source": [
        "## Create (local) `SparkContext`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gA3lW2W86Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_spark_context():\n",
        "    sc = SparkContext(master=\"local[*]\", appName=APP_NAME)\n",
        "    spark = SparkSession(sc)\n",
        "    \n",
        "    return sc, spark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbSCNkzg86Ui",
        "colab_type": "text"
      },
      "source": [
        "## Load the Dataset (Training Set) of Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBo0HDh686Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(dataset_file_path):\n",
        "    # Define the schema of the dataset\n",
        "    tweets_schema = tp.StructType([tp.StructField(name=\"id\", dataType=tp.IntegerType(), nullable=True), \n",
        "                                  tp.StructField(name=\"tweet\", dataType=tp.StringType(), nullable=True), \n",
        "                                  tp.StructField(name=\"label\", dataType=tp.IntegerType(), nullable=True)\n",
        "                             ])\n",
        "    # Loading the data set\n",
        "    tweet_df = spark.read.csv(dataset_file_path, \n",
        "                              schema=tweets_schema, \n",
        "                              header=True)\n",
        "    \n",
        "    return tweet_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL9-215N86Uk",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Stages of our Machine Learning Pipeline\n",
        "\n",
        "Here we define the different stages in which we want to transform the data, and then use it to get the predicted label from our model.\n",
        "\n",
        "This is composed of **4 stages** in total:\n",
        "\n",
        "-   1. [`RegexTokenizer`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer): In the first stage, we will use regular expression to convert each tweet (i.e., text string) into a list of _proper_ words;\n",
        "-  2. [`StopWordsRemover`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover): We will then remove the stop words from the word list obtained before;\n",
        "-  3. [`Word2Vec`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.feature.Word2Vec): We will create _word vectors_ using [Word2Vec](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#word2vec), which is a (shallow) neural-network-based model to map each word to a vector of numbers. Word2Vec is a well-known, standard technique to extract features from raw text, which is alternative to (and generally way better performing than) traditional bag-of-words approach like TF-IDF. For more information on how Word2Vec works, please refer to this [source](https://code.google.com/archive/p/word2vec/).\n",
        "\n",
        "-  4. [`LogisticRegression`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression): In the final stage, we will use word vectors extracted using Word2Vec to build a logistic regression model and get the prediction on whether a tweet contains hate speech or not.\n",
        "\n",
        "**REMEMBER:** Our focus here is not on building a very accurate classification model, but rather to see how can we use a predictive model to get the results on streaming data\n",
        "\n",
        "Below there is an illustration of the pipeline just described:\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/pipeline_streaming.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6-s8YAT86Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ml_pipeline(train):\n",
        "    \n",
        "    print(\"***** Defining the pipeline stages *****\\n\")\n",
        "    \n",
        "    # define stage 1: tokenize the tweet text  \n",
        "    stage_1 = RegexTokenizer(inputCol=\"tweet\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
        "    # define stage 2: remove the stop words\n",
        "    stage_2 = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_words\")\n",
        "    # define stage 3: create a word vector of the size 100\n",
        "    stage_3 = Word2Vec(inputCol=\"filtered_words\", outputCol=\"feature_vector\", vectorSize=100)\n",
        "    # define stage 4: Logistic Regression Model\n",
        "    model = LogisticRegression(featuresCol=\"feature_vector\", labelCol=\"label\") \n",
        "    \n",
        "    print(\"***** Create the corresponding pipeline *****\\n\")\n",
        "    pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, model])\n",
        "\n",
        "    print(\"***** Fit the pipeline to the training data *****\\n\")\n",
        "    pipeline_fit = pipeline.fit(train)\n",
        "    \n",
        "    return pipeline_fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py_z467r86Un",
        "colab_type": "text"
      },
      "source": [
        "## Function Used to Get Online Predictions from Streamed Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3TiCVgj86Uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the function to get the predicted sentiment on the data received\n",
        "def get_prediction(pipeline_fit, tweet_text):\n",
        "    try:\n",
        "        # remove blank tweets\n",
        "        tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
        "        # create the dataframe with each row containing the text of a tweet\n",
        "        row_rdd = tweet_text.map(lambda w: Row(tweet=w))\n",
        "        words_df = spark.createDataFrame(row_rdd)\n",
        "        # get the prediction for each row\n",
        "        pipeline_fit.transform(words_df).select(\"tweet\", \"prediction\").show(truncate=False)\n",
        "    except: \n",
        "        print(\"\\nWaiting for streaming data...\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS2a76yH86Uq",
        "colab_type": "text"
      },
      "source": [
        "## Main Application Entry Point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eipj6ld386Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"\"\"Wrong number of input arguments!\n",
        "Usage: \n",
        "> check-tweet-sentiment.py HOSTNAME PORT\n",
        "where\n",
        "    - HOSTNAME is the hostname of the TCP streaming data source (e.g., localhost)\n",
        "    - PORT is the port number of the TCP streaming data source (e.g., 9999)\"\"\", \n",
        "      file=sys.stderr)\n",
        "        \n",
        "        sys.exit(-1)\n",
        "    \n",
        "    # Save the `hostname` and `port` of the TCP source, where the streaming data will come from\n",
        "    hostname = sys.argv[1]\n",
        "    port = int(sys.argv[2])\n",
        "        \n",
        "    # Create a local SparkContext with 2 working threads (i.e., one for the receiver and the other for processing)\n",
        "    sc, spark = create_spark_context()\n",
        "    \n",
        "    # Setting the locale to \"en-US\" \n",
        "    # By default, the locale is the one of the JVM where Spark is running \n",
        "    # (which, in turn, is the system locale of the host machine where such a JVM is running)\n",
        "    locale = sc._jvm.java.util.Locale\n",
        "    locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
        "    \n",
        "    # Load the dataset of tweets\n",
        "    print(\"***** Loading the training set of tweets... *****\\n\")\n",
        "    tweet_train_df = load_dataset(DATASET_FILE_PATH)\n",
        "    \n",
        "    print(\"=> Dataset schema:\\n\")\n",
        "    tweet_train_df.printSchema()\n",
        "    print(\"=> Dataset excerpt (first 5 rows):\\n\")\n",
        "    tweet_train_df.show(5, truncate=False)\n",
        "    print(\"=> Removing null entries...\\n\")\n",
        "    tweet_train_df = tweet_train_df.na.drop()\n",
        "    print(\"=> Dataset size (n. of rows): {:d}\\n\".format(tweet_train_df.count()))\n",
        "    \n",
        "    # Fit the ML pipeline to the training set of tweets\n",
        "    pipeline_fit = ml_pipeline(tweet_train_df)\n",
        "    \n",
        "    # Test the hate speech prediction model online with incoming streaming data\n",
        "    print(\"***** Model for hate speech detection successfully trained! Now, waiting for the incoming streaming data to classify... *****\\n\")\n",
        "    \n",
        "    # Create the streaming context using the specified batch interval\n",
        "    ssc = StreamingContext(sc, batchDuration=BATCH_INTERVAL_SECS)\n",
        "    \n",
        "    # Get streaming data from the source TCP socket \n",
        "    lines = ssc.socketTextStream(hostname, port)\n",
        "    \n",
        "    # Create the discretized stream from each line starting with \"TWEET\"\n",
        "    words = lines.flatMap(lambda line: line.split(\"TWEET\"))\n",
        "    \n",
        "    # Send each RDD associated with this DStream to the model, and ask it for a prediction on each of them \n",
        "    words.foreachRDD(lambda tweet_rdd: get_prediction(pipeline_fit, tweet_rdd))\n",
        "\n",
        "    # Start the computation\n",
        "    ssc.start()\n",
        "\n",
        "    # Wait for the computation to terminate\n",
        "    ssc.awaitTermination()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
